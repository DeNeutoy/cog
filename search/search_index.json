{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cog: Containers for machine learning # Use Docker for machine learning, without all the pain. What is Cog? Prerequisites Install Upgrade Next steps Need help? Contributors \u2728 What is Cog? # Cog is an open-source command-line tool that gives you a consistent environment to run your model in \u2013 for developing on your laptop, training on GPU machines, and for other people working on the model. Once you've trained your model and you want to share or deploy it, you can bake the model into a Docker image that serves a standard HTTP API and can be deployed anywhere. Cog does a few handy things beyond normal Docker: Automatic Docker image. Define your environment with a simple configuration file, then Cog will generate Dockerfiles with best practices and do all the GPU configuration for you. Automatic HTTP service. Cog will generate an HTTP service from the definition of your model, so you don't need to write a Flask server in the right way. No more CUDA hell. Cog knows which CUDA/cuDNN/PyTorch/Tensorflow/Python combos are compatible and will pick the right versions for you. Develop and train in a consistent environment # Define the Docker environment your model runs in with cog.yaml : build : gpu : true system_packages : - \"libgl1-mesa-glx\" - \"libglib2.0-0\" python_version : \"3.8\" python_packages : - \"torch==1.8.1\" Now, you can run commands inside this environment: $ cog run python train.py ... This will: Generate a Dockerfile with best practices Pick the right CUDA version Build an image Run python train.py in the image with the current directory mounted as a volume and GPUs hooked up correctly Put a trained model in a Docker image # First, you define how predictions are run on your model: import cog import torch class ColorizationPredictor ( cog . Predictor ): def setup ( self ): \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\" self . model = torch . load ( \"./weights.pth\" ) # The arguments and types the model takes as input @cog . input ( \"input\" , type = cog . Path , help = \"Grayscale input image\" ) def predict ( self , input ): \"\"\"Run a single prediction on the model\"\"\" processed_input = preprocess ( input ) output = self . model ( processed_input ) return postprocess ( output ) Now, you can run predictions on this model: $ cog predict -i @input.jpg --> Building Docker image... --> Running Prediction... --> Output written to output.jpg Or, build a Docker image for deployment: $ cog build -t my-colorization-model --> Building Docker image... --> Built my-colorization-model:latest $ docker run -d -p 5000:5000 --gpus all my-colorization-model $ curl http://localhost:5000/predict -X POST -F input=@image.png That's it! Your model will now run forever in this reproducible Docker environment. Why are we building this? # It's really hard for researchers to ship machine learning models to production. Part of the solution is Docker, but it is so complex to get it to work: Dockerfiles, pre-/post-processing, Flask servers, CUDA versions. More often than not the researcher has to sit down with an engineer to get the damn thing deployed. We are Andreas and Ben , and we're trying to fix this. Andreas used to work at Spotify, where he built tools for building and deploying ML models with Docker. Ben worked at Docker, where he created Docker Compose . We realized that, in addition to Spotify, other companies were also using Docker to build and deploy machine learning models. Uber , Coinbase, and others have built similar systems. So, we're making an open source version so other people can do this too. Hit us up if you're interested in using it or want to collaborate with us. We're on Discord or email us at team@replicate.com . Prerequisites # macOS or Linux . Cog works on macOS and Linux, but does not currently support Windows. Docker . Cog uses Docker to create a container for your model. You'll need to install Docker before you can run Cog. Install # First, install Docker if you haven't already . Then, run this in a terminal: sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m` sudo chmod +x /usr/local/bin/cog Upgrade # If you're already got Cog installed and want to update to a newer version: sudo rm $(which cog) sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m` sudo chmod +x /usr/local/bin/cog Next steps # Get started with an example model Get started with your own model Take a look at some examples of using Cog cog.yaml reference to learn how to define your model's environment Prediction interface reference to learn how the cog.Predictor interface works Need help? # Join us in #cog on Discord. Contributors \u2728 # Thanks goes to these wonderful people ( emoji key ): Ben Firshman \ud83d\udcbb \ud83d\udcd6 Andreas Jansson \ud83d\udcbb \ud83d\udcd6 Zeke Sikelianos \ud83d\udcbb \ud83d\udcd6 \ud83d\udd27 Rory Byrne \ud83d\udcbb \ud83d\udcd6 Michael Floering \ud83d\udcbb \ud83d\udcd6 \ud83e\udd14 Ben Evans \ud83d\udcd6 shashank agarwal \ud83d\udcbb \ud83d\udcd6 VictorXLR \ud83d\udcbb \ud83d\udcd6 \u26a0\ufe0f hung anna \ud83d\udc1b Brian Whitman \ud83d\udc1b JimothyJohn \ud83d\udc1b ericguizzo \ud83d\udc1b Dominic Baggott \ud83d\udcbb \u26a0\ufe0f This project follows the all-contributors specification. Contributions of any kind welcome!","title":"Cog"},{"location":"#cog-containers-for-machine-learning","text":"Use Docker for machine learning, without all the pain. What is Cog? Prerequisites Install Upgrade Next steps Need help? Contributors \u2728","title":"Cog: Containers for machine learning"},{"location":"#what-is-cog","text":"Cog is an open-source command-line tool that gives you a consistent environment to run your model in \u2013 for developing on your laptop, training on GPU machines, and for other people working on the model. Once you've trained your model and you want to share or deploy it, you can bake the model into a Docker image that serves a standard HTTP API and can be deployed anywhere. Cog does a few handy things beyond normal Docker: Automatic Docker image. Define your environment with a simple configuration file, then Cog will generate Dockerfiles with best practices and do all the GPU configuration for you. Automatic HTTP service. Cog will generate an HTTP service from the definition of your model, so you don't need to write a Flask server in the right way. No more CUDA hell. Cog knows which CUDA/cuDNN/PyTorch/Tensorflow/Python combos are compatible and will pick the right versions for you.","title":"What is Cog?"},{"location":"#develop-and-train-in-a-consistent-environment","text":"Define the Docker environment your model runs in with cog.yaml : build : gpu : true system_packages : - \"libgl1-mesa-glx\" - \"libglib2.0-0\" python_version : \"3.8\" python_packages : - \"torch==1.8.1\" Now, you can run commands inside this environment: $ cog run python train.py ... This will: Generate a Dockerfile with best practices Pick the right CUDA version Build an image Run python train.py in the image with the current directory mounted as a volume and GPUs hooked up correctly","title":"Develop and train in a consistent environment"},{"location":"#put-a-trained-model-in-a-docker-image","text":"First, you define how predictions are run on your model: import cog import torch class ColorizationPredictor ( cog . Predictor ): def setup ( self ): \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\" self . model = torch . load ( \"./weights.pth\" ) # The arguments and types the model takes as input @cog . input ( \"input\" , type = cog . Path , help = \"Grayscale input image\" ) def predict ( self , input ): \"\"\"Run a single prediction on the model\"\"\" processed_input = preprocess ( input ) output = self . model ( processed_input ) return postprocess ( output ) Now, you can run predictions on this model: $ cog predict -i @input.jpg --> Building Docker image... --> Running Prediction... --> Output written to output.jpg Or, build a Docker image for deployment: $ cog build -t my-colorization-model --> Building Docker image... --> Built my-colorization-model:latest $ docker run -d -p 5000:5000 --gpus all my-colorization-model $ curl http://localhost:5000/predict -X POST -F input=@image.png That's it! Your model will now run forever in this reproducible Docker environment.","title":"Put a trained model in a Docker image"},{"location":"#why-are-we-building-this","text":"It's really hard for researchers to ship machine learning models to production. Part of the solution is Docker, but it is so complex to get it to work: Dockerfiles, pre-/post-processing, Flask servers, CUDA versions. More often than not the researcher has to sit down with an engineer to get the damn thing deployed. We are Andreas and Ben , and we're trying to fix this. Andreas used to work at Spotify, where he built tools for building and deploying ML models with Docker. Ben worked at Docker, where he created Docker Compose . We realized that, in addition to Spotify, other companies were also using Docker to build and deploy machine learning models. Uber , Coinbase, and others have built similar systems. So, we're making an open source version so other people can do this too. Hit us up if you're interested in using it or want to collaborate with us. We're on Discord or email us at team@replicate.com .","title":"Why are we building this?"},{"location":"#prerequisites","text":"macOS or Linux . Cog works on macOS and Linux, but does not currently support Windows. Docker . Cog uses Docker to create a container for your model. You'll need to install Docker before you can run Cog.","title":"Prerequisites"},{"location":"#install","text":"First, install Docker if you haven't already . Then, run this in a terminal: sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m` sudo chmod +x /usr/local/bin/cog","title":"Install"},{"location":"#upgrade","text":"If you're already got Cog installed and want to update to a newer version: sudo rm $(which cog) sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m` sudo chmod +x /usr/local/bin/cog","title":"Upgrade"},{"location":"#next-steps","text":"Get started with an example model Get started with your own model Take a look at some examples of using Cog cog.yaml reference to learn how to define your model's environment Prediction interface reference to learn how the cog.Predictor interface works","title":"Next steps"},{"location":"#need-help","text":"Join us in #cog on Discord.","title":"Need help?"},{"location":"#contributors","text":"Thanks goes to these wonderful people ( emoji key ): Ben Firshman \ud83d\udcbb \ud83d\udcd6 Andreas Jansson \ud83d\udcbb \ud83d\udcd6 Zeke Sikelianos \ud83d\udcbb \ud83d\udcd6 \ud83d\udd27 Rory Byrne \ud83d\udcbb \ud83d\udcd6 Michael Floering \ud83d\udcbb \ud83d\udcd6 \ud83e\udd14 Ben Evans \ud83d\udcd6 shashank agarwal \ud83d\udcbb \ud83d\udcd6 VictorXLR \ud83d\udcbb \ud83d\udcd6 \u26a0\ufe0f hung anna \ud83d\udc1b Brian Whitman \ud83d\udc1b JimothyJohn \ud83d\udc1b ericguizzo \ud83d\udc1b Dominic Baggott \ud83d\udcbb \u26a0\ufe0f This project follows the all-contributors specification. Contributions of any kind welcome!","title":"Contributors \u2728"},{"location":"CONTRIBUTING/","text":"Contributing guide # Making a contribution # Signing your work # Each commit you contribute to Cog must be signed off (not to be confused with signing ). It certifies that you wrote the patch, or have the right to contribute it. It is called the Developer Certificate of Origin and was originally developed for the Linux kernel. If you can certify the following: By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. Then add this line to each of your Git commit messages, with your name and email: Signed-off-by: Sam Smith <sam.smith@example.com> How to sign off your commits # If you're using the git CLI, you can sign a commit by passing the -s option: git commit -s -m \"Reticulate splines\" You can also create a git hook which will sign off all your commits automatically. Using hooks also allows you to sign off commits when using non-command-line tools like GitHub Desktop or VS Code. First, create the hook file and make it executable: cd your/checkout/of/cog touch .git/hooks/prepare-commit-msg chmod +x .git/hooks/prepare-commit-msg Then paste the following into the file: #!/bin/sh NAME=$(git config user.name) EMAIL=$(git config user.email) if [ -z \"$NAME\" ]; then echo \"empty git config user.name\" exit 1 fi if [ -z \"$EMAIL\" ]; then echo \"empty git config user.email\" exit 1 fi git interpret-trailers --if-exists doNothing --trailer \\ \"Signed-off-by: $NAME <$EMAIL>\" \\ --in-place \"$1\" Acknowledging contributions # We welcome contributions from everyone, and consider all forms of contribution equally valuable. This includes code, bug reports, feature requests, and documentation. We use All Contributors to maintain a list of all the people who have contributed to Cog. To acknowledge a contribution, add a comment to an issue or pull request in the following format: @allcontributors please add @username for doc,code,ideas A bot will automatically open a pull requests to add the contributor to the project README. Common contribution types include: doc , code , bug , and ideas . See the full list at allcontributors.org/docs/en/emoji-key Development environment # You'll need to install Go 1.16 . If you're using a newer Mac with an M1 chip, be sure to download the darwin-arm64 installer package. Alternatively you can run brew install go which will automatically detect and use the appropriate installer for your system architecture. Once you have Go installed, then run: make install This installs the cog binary to $GOPATH/bin/cog . To run the tests: make test The project is formatted by goimports. To format the source code, run: make fmt If you encounter any errors, see the troubleshooting section below? Project structure # As much as possible, this is attempting to follow the Standard Go Project Layout . cmd/ - The root cog command. pkg/cli/ - CLI commands. pkg/config - Everything cog.yaml related. pkg/docker/ - Low-level interface for Docker commands. pkg/dockerfile/ - Creates Dockerfiles. pkg/image/ - Creates and manipulates Cog Docker images. pkg/predict/ - Runs predictions on models. pkg/util/ - Various packages that aren't part of Cog. They could reasonably be separate re-usable projects. python/ - The Cog Python library. test-integration/ - High-level integration tests for Cog. Publishing a release # This project has a GitHub Actions workflow that uses goreleaser to facilitate the process of publishing new releases. The release process is triggered by manually creating and pushing a new git tag. To publish a new release, run the following in your local checkout of cog: git checkout main git fetch --all --tags git tag v0.0.11 git push --tags Then visit github.com/replicate/cog/actions to monitor the release process. Troubleshooting # invalid command 'bdist_wheel' # If you get this error running make install , you'll need to run pip install wheel first. cog command not found # The compiled cog binary will be installed in $GOPATH/bin/cog , e.g. ~/go/bin/cog . Make sure that Golang's bin directory is present on your system PATH by adding it to your shell config ( .bashrc , .zshrc , etc): export PATH =~/ go / bin : $ PATH Still having trouble? Please open an issue on GitHub.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-guide","text":"","title":"Contributing guide"},{"location":"CONTRIBUTING/#making-a-contribution","text":"","title":"Making a contribution"},{"location":"CONTRIBUTING/#signing-your-work","text":"Each commit you contribute to Cog must be signed off (not to be confused with signing ). It certifies that you wrote the patch, or have the right to contribute it. It is called the Developer Certificate of Origin and was originally developed for the Linux kernel. If you can certify the following: By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. Then add this line to each of your Git commit messages, with your name and email: Signed-off-by: Sam Smith <sam.smith@example.com>","title":"Signing your work"},{"location":"CONTRIBUTING/#how-to-sign-off-your-commits","text":"If you're using the git CLI, you can sign a commit by passing the -s option: git commit -s -m \"Reticulate splines\" You can also create a git hook which will sign off all your commits automatically. Using hooks also allows you to sign off commits when using non-command-line tools like GitHub Desktop or VS Code. First, create the hook file and make it executable: cd your/checkout/of/cog touch .git/hooks/prepare-commit-msg chmod +x .git/hooks/prepare-commit-msg Then paste the following into the file: #!/bin/sh NAME=$(git config user.name) EMAIL=$(git config user.email) if [ -z \"$NAME\" ]; then echo \"empty git config user.name\" exit 1 fi if [ -z \"$EMAIL\" ]; then echo \"empty git config user.email\" exit 1 fi git interpret-trailers --if-exists doNothing --trailer \\ \"Signed-off-by: $NAME <$EMAIL>\" \\ --in-place \"$1\"","title":"How to sign off your commits"},{"location":"CONTRIBUTING/#acknowledging-contributions","text":"We welcome contributions from everyone, and consider all forms of contribution equally valuable. This includes code, bug reports, feature requests, and documentation. We use All Contributors to maintain a list of all the people who have contributed to Cog. To acknowledge a contribution, add a comment to an issue or pull request in the following format: @allcontributors please add @username for doc,code,ideas A bot will automatically open a pull requests to add the contributor to the project README. Common contribution types include: doc , code , bug , and ideas . See the full list at allcontributors.org/docs/en/emoji-key","title":"Acknowledging contributions"},{"location":"CONTRIBUTING/#development-environment","text":"You'll need to install Go 1.16 . If you're using a newer Mac with an M1 chip, be sure to download the darwin-arm64 installer package. Alternatively you can run brew install go which will automatically detect and use the appropriate installer for your system architecture. Once you have Go installed, then run: make install This installs the cog binary to $GOPATH/bin/cog . To run the tests: make test The project is formatted by goimports. To format the source code, run: make fmt If you encounter any errors, see the troubleshooting section below?","title":"Development environment"},{"location":"CONTRIBUTING/#project-structure","text":"As much as possible, this is attempting to follow the Standard Go Project Layout . cmd/ - The root cog command. pkg/cli/ - CLI commands. pkg/config - Everything cog.yaml related. pkg/docker/ - Low-level interface for Docker commands. pkg/dockerfile/ - Creates Dockerfiles. pkg/image/ - Creates and manipulates Cog Docker images. pkg/predict/ - Runs predictions on models. pkg/util/ - Various packages that aren't part of Cog. They could reasonably be separate re-usable projects. python/ - The Cog Python library. test-integration/ - High-level integration tests for Cog.","title":"Project structure"},{"location":"CONTRIBUTING/#publishing-a-release","text":"This project has a GitHub Actions workflow that uses goreleaser to facilitate the process of publishing new releases. The release process is triggered by manually creating and pushing a new git tag. To publish a new release, run the following in your local checkout of cog: git checkout main git fetch --all --tags git tag v0.0.11 git push --tags Then visit github.com/replicate/cog/actions to monitor the release process.","title":"Publishing a release"},{"location":"CONTRIBUTING/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"CONTRIBUTING/#invalid-command-bdist_wheel","text":"If you get this error running make install , you'll need to run pip install wheel first.","title":"invalid command 'bdist_wheel'"},{"location":"CONTRIBUTING/#cog-command-not-found","text":"The compiled cog binary will be installed in $GOPATH/bin/cog , e.g. ~/go/bin/cog . Make sure that Golang's bin directory is present on your system PATH by adding it to your shell config ( .bashrc , .zshrc , etc): export PATH =~/ go / bin : $ PATH Still having trouble? Please open an issue on GitHub.","title":"cog command not found"},{"location":"getting-started-own-model/","text":"Getting started with your own model # This guide will show you how to put your own machine learning model in a Docker image using Cog. If you haven't got a model to try out, you'll want to follow the main getting started guide . Prerequisites # macOS or Linux . Cog works on macOS and Linux, but does not currently support Windows. Docker . Cog uses Docker to create a container for your model. You'll need to install Docker before you can run Cog. Initialization # First, install Cog if you haven't already: sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_ ` uname -s ` _ ` uname -m ` sudo chmod +x /usr/local/bin/cog To configure your project for use with Cog, you'll need to add two files: cog.yaml defines system requirements, Python package dependencies, etc predict.py describes the prediction interface for your model Use the cog init command to generate these files in your project: $ cd path/to/your/model $ cog init Define the Docker environment # The cog.yaml file defines all the different things that need to be installed for your model to run. You can think of it as a simple way of defining a Docker image. For example: build : python_version : \"3.8\" python_packages : - \"torch==1.7.0\" This will generate a Docker image with Python 3.8 and PyTorch 1.7 installed, for both CPU and GPU, with the correct version of CUDA, and various other sensible best-practices. To run a command inside this environment, prefix it with cog run : $ cog run python \u2713 Building Docker image from cog.yaml... Successfully built 8f54020c8981 Running 'python' in Docker with the current directory mounted as a volume... \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Python 3.8.10 (default, May 12 2021, 23:32:14) [GCC 9.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> This is handy for ensuring a consistent environment for development or training. With cog.yaml , you can also install system packages and other things. Take a look at the full reference to see what else you can do. Define how to run predictions # The next step is to update predict.py to define the interface for running predictions on your model. The predict.py generated by cog init looks something like this: import cog from pathlib import Path import torch class Predictor ( cog . Predictor ): def setup ( self ): \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\" self . net = torch . load ( \"weights.pth\" ) # Define the input types for a prediction @cog . input ( \"input\" , type = Path , help = \"Image to enlarge\" ) @cog . input ( \"scale\" , type = float , default = 1.5 , help = \"Factor to scale image by\" ) def predict ( self , input , scale ): \"\"\"Run a single prediction on the model\"\"\" # ... pre-processing ... output = self . net ( input ) # ... post-processing ... return output Edit your predict.py file and fill in the functions with your own model's setup and prediction code. You might need to import parts of your model from another file. You also need to define the inputs to your model using the @cog.input() decorator, as demonstrated above. The first argument maps to the name of the argument in the predict() function, and it also takes these other arguments: type : Either str , int , float , bool , or Path (be sure to add the import, as in the example above). Path is used for files. For more complex inputs, save it to a file and use Path . help : A description of what to pass to this input for users of the model default : A default value to set the input to. If this argument is not passed, the input is required. If it is explicitly set to None , the input is optional. min : A minimum value for int or float types. max : A maximum value for int or float types. options : A list of values to limit the input to. It can be used with str , int , and float inputs. For more details about writing your model interface, take a look at the prediction interface documentation . Next, add the line predict: \"predict.py:Predictor\" to your cog.yaml , so it looks something like this: build : python_version : \"3.8\" python_packages : - \"torch==1.7.0\" predict : \"predict.py:Predictor\" That's it! To test this works, try running a prediction on the model: $ cog predict -i input=@input.jpg \u2713 Building Docker image from cog.yaml... Successfully built 664ef88bc1f4 \u2713 Model running in Docker image 664ef88bc1f4 Written output to output.png To pass more inputs to the model, you can add more -i options: $ cog predict -i input=@input.jpg -i scale=2.0 In this case it is just a number, not a file, so you don't need the @ prefix. Using GPUs # To use GPUs with Cog, add the gpu: true option to the build section of your cog.yaml : build : gpu : true ... Cog will use the nvidia-docker base image and automatically figure out what versions of CUDA and cuDNN to use based on the version of Python, PyTorch, and Tensorflow that you are using. For more details, see the gpu section of the cog.yaml reference . Next steps # Next, you might want to take a look at: The reference for cog.yaml The reference for the Python library","title":"Using your own Model"},{"location":"getting-started-own-model/#getting-started-with-your-own-model","text":"This guide will show you how to put your own machine learning model in a Docker image using Cog. If you haven't got a model to try out, you'll want to follow the main getting started guide .","title":"Getting started with your own model"},{"location":"getting-started-own-model/#prerequisites","text":"macOS or Linux . Cog works on macOS and Linux, but does not currently support Windows. Docker . Cog uses Docker to create a container for your model. You'll need to install Docker before you can run Cog.","title":"Prerequisites"},{"location":"getting-started-own-model/#initialization","text":"First, install Cog if you haven't already: sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_ ` uname -s ` _ ` uname -m ` sudo chmod +x /usr/local/bin/cog To configure your project for use with Cog, you'll need to add two files: cog.yaml defines system requirements, Python package dependencies, etc predict.py describes the prediction interface for your model Use the cog init command to generate these files in your project: $ cd path/to/your/model $ cog init","title":"Initialization"},{"location":"getting-started-own-model/#define-the-docker-environment","text":"The cog.yaml file defines all the different things that need to be installed for your model to run. You can think of it as a simple way of defining a Docker image. For example: build : python_version : \"3.8\" python_packages : - \"torch==1.7.0\" This will generate a Docker image with Python 3.8 and PyTorch 1.7 installed, for both CPU and GPU, with the correct version of CUDA, and various other sensible best-practices. To run a command inside this environment, prefix it with cog run : $ cog run python \u2713 Building Docker image from cog.yaml... Successfully built 8f54020c8981 Running 'python' in Docker with the current directory mounted as a volume... \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Python 3.8.10 (default, May 12 2021, 23:32:14) [GCC 9.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> This is handy for ensuring a consistent environment for development or training. With cog.yaml , you can also install system packages and other things. Take a look at the full reference to see what else you can do.","title":"Define the Docker environment"},{"location":"getting-started-own-model/#define-how-to-run-predictions","text":"The next step is to update predict.py to define the interface for running predictions on your model. The predict.py generated by cog init looks something like this: import cog from pathlib import Path import torch class Predictor ( cog . Predictor ): def setup ( self ): \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\" self . net = torch . load ( \"weights.pth\" ) # Define the input types for a prediction @cog . input ( \"input\" , type = Path , help = \"Image to enlarge\" ) @cog . input ( \"scale\" , type = float , default = 1.5 , help = \"Factor to scale image by\" ) def predict ( self , input , scale ): \"\"\"Run a single prediction on the model\"\"\" # ... pre-processing ... output = self . net ( input ) # ... post-processing ... return output Edit your predict.py file and fill in the functions with your own model's setup and prediction code. You might need to import parts of your model from another file. You also need to define the inputs to your model using the @cog.input() decorator, as demonstrated above. The first argument maps to the name of the argument in the predict() function, and it also takes these other arguments: type : Either str , int , float , bool , or Path (be sure to add the import, as in the example above). Path is used for files. For more complex inputs, save it to a file and use Path . help : A description of what to pass to this input for users of the model default : A default value to set the input to. If this argument is not passed, the input is required. If it is explicitly set to None , the input is optional. min : A minimum value for int or float types. max : A maximum value for int or float types. options : A list of values to limit the input to. It can be used with str , int , and float inputs. For more details about writing your model interface, take a look at the prediction interface documentation . Next, add the line predict: \"predict.py:Predictor\" to your cog.yaml , so it looks something like this: build : python_version : \"3.8\" python_packages : - \"torch==1.7.0\" predict : \"predict.py:Predictor\" That's it! To test this works, try running a prediction on the model: $ cog predict -i input=@input.jpg \u2713 Building Docker image from cog.yaml... Successfully built 664ef88bc1f4 \u2713 Model running in Docker image 664ef88bc1f4 Written output to output.png To pass more inputs to the model, you can add more -i options: $ cog predict -i input=@input.jpg -i scale=2.0 In this case it is just a number, not a file, so you don't need the @ prefix.","title":"Define how to run predictions"},{"location":"getting-started-own-model/#using-gpus","text":"To use GPUs with Cog, add the gpu: true option to the build section of your cog.yaml : build : gpu : true ... Cog will use the nvidia-docker base image and automatically figure out what versions of CUDA and cuDNN to use based on the version of Python, PyTorch, and Tensorflow that you are using. For more details, see the gpu section of the cog.yaml reference .","title":"Using GPUs"},{"location":"getting-started-own-model/#next-steps","text":"Next, you might want to take a look at: The reference for cog.yaml The reference for the Python library","title":"Next steps"},{"location":"getting-started/","text":"Getting started # This guide will walk you through what you can do with Cog by using an example model. Prerequisites # macOS or Linux . Cog works on macOS and Linux, but does not currently support Windows. Docker . Cog uses Docker to create a container for your model. You'll need to install Docker before you can run Cog. Install Cog # First, install Cog: sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_ ` uname -s ` _ ` uname -m ` sudo chmod +x /usr/local/bin/cog Create a project # Let's make a directory to work in: mkdir cog-quickstart cd cog-quickstart Run commands # The simplest thing you can do with Cog is run a command inside a Docker environment. The first thing you need to do is create a file called cog.yaml : build : python_version : \"3.8\" Then, you can run any command inside this environment. For example, to get a Python shell: $ cog run python \u2713 Building Docker image from cog.yaml... Successfully built 8f54020c8981 Running 'python' in Docker with the current directory mounted as a volume... \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Python 3 .8.10 ( default, May 12 2021 , 23 :32:14 ) [ GCC 9 .3.0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Inside this environment you can do anything \u2013 run a Jupyter notebook, your training script, your evaluation script, and so on. Run predictions on a model # Let's pretend we've trained a model. With Cog, we can define how to run predictions on it in a standard way, so other people can easily run predictions on it without having to hunt around for a prediction script. First, run this to get some pre-trained model weights: curl -O https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5 Then, we need to write some code to describe how predictions are run on the model. Save this to predict.py : import cog from pathlib import Path from tensorflow.keras.applications.resnet50 import ResNet50 from tensorflow.keras.preprocessing import image from tensorflow.keras.applications.resnet50 import preprocess_input , decode_predictions import numpy as np class ResNetPredictor ( cog . Predictor ): def setup ( self ): \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\" self . model = ResNet50 ( weights = 'resnet50_weights_tf_dim_ordering_tf_kernels.h5' ) # Define the arguments and types the model takes as input @cog . input ( \"input\" , type = Path , help = \"Image to classify\" ) def predict ( self , input ): \"\"\"Run a single prediction on the model\"\"\" # Preprocess the image img = image . load_img ( input , target_size = ( 224 , 224 )) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) x = preprocess_input ( x ) # Run the prediction preds = self . model . predict ( x ) # Return the top 3 predictions return str ( decode_predictions ( preds , top = 3 )[ 0 ]) We also need to point Cog at this, and tell it what Python dependencies to install. Update cog.yaml to look like this: build : python_version : \"3.8\" python_packages : - pillow==8.3.1 - tensorflow==2.5.0 predict : \"predict.py:ResNetPredictor\" Let's grab an image to test the model with: curl https : // upload . wikimedia . org / wikipedia / commons / 4 / 4 d / Cat_November_2010 - 1 a . jpg > input . jpg Now, let's run the model using Cog: $ cog predict -i @input.jpg ... [ [ \"n02123159\", \"tiger_cat\", 0.4874822497367859 ], [ \"n02123045\", \"tabby\", 0.23169134557247162 ], [ \"n02124075\", \"Egyptian_cat\", 0.09728282690048218 ] ] Looks like it worked! Note: The first time you run cog predict , the build process will be triggered to generate a Docker container that can run your model. The next time you run cog predict the pre-built container will be used. Build an image # We can bake your model's code, the trained weights, and the Docker environment into a Docker image. This image serves predictions with an HTTP server, and can be deployed to anywhere that Docker runs to serve real-time predictions. $ cog build -t resnet Building Docker image... Built resnet:latest You can run this image with cog predict by passing the image name as an argument: $ cog predict resnet:latest -i @input.jpg Or, you can run it with Docker directly, and it'll serve an HTTP server: $ docker run -d -p 5000:5000 --gpus all resnet $ curl http://localhost:5000/predict -X POST -F input=@image.png As a shorthand, you can add the image name as an extra line in cog.yaml : image : \"r8.im/replicate/resnet\" Once you've done this, you can use cog push to build and push the image to a Docker registry: $ cog push Building r8.im/replicate/resnet... Pushing r8.im/replicate/resnet... Pushed! The Docker image is now accessible to anyone or any system that has access to this Docker registry. Next steps # Those are the basics! Next, you might want to take a look at: A guide to help you set up your own model on Cog. Reference for cog.yaml Reference for the Python library","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This guide will walk you through what you can do with Cog by using an example model.","title":"Getting started"},{"location":"getting-started/#prerequisites","text":"macOS or Linux . Cog works on macOS and Linux, but does not currently support Windows. Docker . Cog uses Docker to create a container for your model. You'll need to install Docker before you can run Cog.","title":"Prerequisites"},{"location":"getting-started/#install-cog","text":"First, install Cog: sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_ ` uname -s ` _ ` uname -m ` sudo chmod +x /usr/local/bin/cog","title":"Install Cog"},{"location":"getting-started/#create-a-project","text":"Let's make a directory to work in: mkdir cog-quickstart cd cog-quickstart","title":"Create a project"},{"location":"getting-started/#run-commands","text":"The simplest thing you can do with Cog is run a command inside a Docker environment. The first thing you need to do is create a file called cog.yaml : build : python_version : \"3.8\" Then, you can run any command inside this environment. For example, to get a Python shell: $ cog run python \u2713 Building Docker image from cog.yaml... Successfully built 8f54020c8981 Running 'python' in Docker with the current directory mounted as a volume... \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Python 3 .8.10 ( default, May 12 2021 , 23 :32:14 ) [ GCC 9 .3.0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Inside this environment you can do anything \u2013 run a Jupyter notebook, your training script, your evaluation script, and so on.","title":"Run commands"},{"location":"getting-started/#run-predictions-on-a-model","text":"Let's pretend we've trained a model. With Cog, we can define how to run predictions on it in a standard way, so other people can easily run predictions on it without having to hunt around for a prediction script. First, run this to get some pre-trained model weights: curl -O https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5 Then, we need to write some code to describe how predictions are run on the model. Save this to predict.py : import cog from pathlib import Path from tensorflow.keras.applications.resnet50 import ResNet50 from tensorflow.keras.preprocessing import image from tensorflow.keras.applications.resnet50 import preprocess_input , decode_predictions import numpy as np class ResNetPredictor ( cog . Predictor ): def setup ( self ): \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\" self . model = ResNet50 ( weights = 'resnet50_weights_tf_dim_ordering_tf_kernels.h5' ) # Define the arguments and types the model takes as input @cog . input ( \"input\" , type = Path , help = \"Image to classify\" ) def predict ( self , input ): \"\"\"Run a single prediction on the model\"\"\" # Preprocess the image img = image . load_img ( input , target_size = ( 224 , 224 )) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) x = preprocess_input ( x ) # Run the prediction preds = self . model . predict ( x ) # Return the top 3 predictions return str ( decode_predictions ( preds , top = 3 )[ 0 ]) We also need to point Cog at this, and tell it what Python dependencies to install. Update cog.yaml to look like this: build : python_version : \"3.8\" python_packages : - pillow==8.3.1 - tensorflow==2.5.0 predict : \"predict.py:ResNetPredictor\" Let's grab an image to test the model with: curl https : // upload . wikimedia . org / wikipedia / commons / 4 / 4 d / Cat_November_2010 - 1 a . jpg > input . jpg Now, let's run the model using Cog: $ cog predict -i @input.jpg ... [ [ \"n02123159\", \"tiger_cat\", 0.4874822497367859 ], [ \"n02123045\", \"tabby\", 0.23169134557247162 ], [ \"n02124075\", \"Egyptian_cat\", 0.09728282690048218 ] ] Looks like it worked! Note: The first time you run cog predict , the build process will be triggered to generate a Docker container that can run your model. The next time you run cog predict the pre-built container will be used.","title":"Run predictions on a model"},{"location":"getting-started/#build-an-image","text":"We can bake your model's code, the trained weights, and the Docker environment into a Docker image. This image serves predictions with an HTTP server, and can be deployed to anywhere that Docker runs to serve real-time predictions. $ cog build -t resnet Building Docker image... Built resnet:latest You can run this image with cog predict by passing the image name as an argument: $ cog predict resnet:latest -i @input.jpg Or, you can run it with Docker directly, and it'll serve an HTTP server: $ docker run -d -p 5000:5000 --gpus all resnet $ curl http://localhost:5000/predict -X POST -F input=@image.png As a shorthand, you can add the image name as an extra line in cog.yaml : image : \"r8.im/replicate/resnet\" Once you've done this, you can use cog push to build and push the image to a Docker registry: $ cog push Building r8.im/replicate/resnet... Pushing r8.im/replicate/resnet... Pushed! The Docker image is now accessible to anyone or any system that has access to this Docker registry.","title":"Build an image"},{"location":"getting-started/#next-steps","text":"Those are the basics! Next, you might want to take a look at: A guide to help you set up your own model on Cog. Reference for cog.yaml Reference for the Python library","title":"Next steps"},{"location":"python/","text":"Prediction interface reference # You define how Cog runs predictions on your model by defining a class that inherits from cog.Predictor . It looks something like this: import cog from pathlib import Path import torch class ImageScalingPredictor ( cog . Predictor ): def setup ( self ): self . model = torch . load ( \"weights.pth\" ) @cog . input ( \"input\" , type = Path , help = \"Image to enlarge\" ) @cog . input ( \"scale\" , type = float , default = 1.5 , help = \"Factor to scale image by\" ) def predict ( self , input ): # ... pre-processing ... output = self . model ( input ) # ... post-processing ... return output Tip: Run cog init to generate an annotated predict.py file that can be used as a starting point for setting up your model. You need to override two functions: setup() and predict() . Predictor.setup() # Set up the model for prediction so multiple predictions run efficiently. Include any expensive one-off operations in here like loading trained models, instantiate data transformations, etc. It's best not to download model weights or any other files in this function. You should bake these into the image when you build it. This means your model doesn't depend on any other system being available and accessible. It also means the Docker image ID becomes an immutable identifier for the precise model you're running, instead of the combination of the image ID and whatever files it might have downloaded. Predictor.predict(**kwargs) # Run a single prediction. This is where you call the model that was loaded during setup() , but you may also want to add pre- and post-processing code here. The predict() function takes an arbitrary list of named arguments, where each argument name must correspond to a @cog.input() annotation. predict() can output strings, numbers, pathlib.Path objects, or lists or dicts of those types. We are working on support for other types of output, but for now we recommend using base-64 encoded strings or pathlib.Path s for more complex outputs. Returning pathlib.Path objects # If the output is a pathlib.Path object, that will be returned by the built-in HTTP server as a file download. To output pathlib.Path objects the file needs to exist, which means that you probably need to create a temporary file first. This file will automatically be deleted by Cog after it has been returned. For example: def predict ( self , input ): output = do_some_processing ( input ) out_path = Path ( tempfile . mkdtemp ()) / \"my-file.txt\" out_path . write_text ( output ) return out_path @cog.input(name, type, help, default=None, min=None, max=None, options=None) # The @cog.input() annotation describes a single input to the predict() function. The name must correspond to an argument name in predict() . It takes these arguments: type : Either str , int , float , bool , or Path (be sure to add the import, as in the example above). Path is used for files. For more complex inputs, save it to a file and use Path . help : A description of what to pass to this input for users of the model default : A default value to set the input to. If this argument is not passed, the input is required. If it is explicitly set to None , the input is optional. min : A minimum value for int or float types. max : A maximum value for int or float types. options : A list of values to limit the input to. It can be used with str , int , and float inputs.","title":"Cog Python API"},{"location":"python/#prediction-interface-reference","text":"You define how Cog runs predictions on your model by defining a class that inherits from cog.Predictor . It looks something like this: import cog from pathlib import Path import torch class ImageScalingPredictor ( cog . Predictor ): def setup ( self ): self . model = torch . load ( \"weights.pth\" ) @cog . input ( \"input\" , type = Path , help = \"Image to enlarge\" ) @cog . input ( \"scale\" , type = float , default = 1.5 , help = \"Factor to scale image by\" ) def predict ( self , input ): # ... pre-processing ... output = self . model ( input ) # ... post-processing ... return output Tip: Run cog init to generate an annotated predict.py file that can be used as a starting point for setting up your model. You need to override two functions: setup() and predict() .","title":"Prediction interface reference"},{"location":"python/#predictorsetup","text":"Set up the model for prediction so multiple predictions run efficiently. Include any expensive one-off operations in here like loading trained models, instantiate data transformations, etc. It's best not to download model weights or any other files in this function. You should bake these into the image when you build it. This means your model doesn't depend on any other system being available and accessible. It also means the Docker image ID becomes an immutable identifier for the precise model you're running, instead of the combination of the image ID and whatever files it might have downloaded.","title":"Predictor.setup()"},{"location":"python/#predictorpredictkwargs","text":"Run a single prediction. This is where you call the model that was loaded during setup() , but you may also want to add pre- and post-processing code here. The predict() function takes an arbitrary list of named arguments, where each argument name must correspond to a @cog.input() annotation. predict() can output strings, numbers, pathlib.Path objects, or lists or dicts of those types. We are working on support for other types of output, but for now we recommend using base-64 encoded strings or pathlib.Path s for more complex outputs.","title":"Predictor.predict(**kwargs)"},{"location":"python/#returning-pathlibpath-objects","text":"If the output is a pathlib.Path object, that will be returned by the built-in HTTP server as a file download. To output pathlib.Path objects the file needs to exist, which means that you probably need to create a temporary file first. This file will automatically be deleted by Cog after it has been returned. For example: def predict ( self , input ): output = do_some_processing ( input ) out_path = Path ( tempfile . mkdtemp ()) / \"my-file.txt\" out_path . write_text ( output ) return out_path","title":"Returning pathlib.Path objects"},{"location":"python/#coginputname-type-help-defaultnone-minnone-maxnone-optionsnone","text":"The @cog.input() annotation describes a single input to the predict() function. The name must correspond to an argument name in predict() . It takes these arguments: type : Either str , int , float , bool , or Path (be sure to add the import, as in the example above). Path is used for files. For more complex inputs, save it to a file and use Path . help : A description of what to pass to this input for users of the model default : A default value to set the input to. If this argument is not passed, the input is required. If it is explicitly set to None , the input is optional. min : A minimum value for int or float types. max : A maximum value for int or float types. options : A list of values to limit the input to. It can be used with str , int , and float inputs.","title":"@cog.input(name, type, help, default=None, min=None, max=None, options=None)"},{"location":"yaml/","text":"cog.yaml reference # cog.yaml defines how to build a Docker image and how to run predictions on your model inside that image. It has three keys: build , image , and predict . It looks a bit like this: build : python_version : \"3.8\" python_packages : - pytorch==1.4.0 system_packages : - \"ffmpeg\" - \"libavcodec-dev\" predict : \"predict.py:JazzSoloComposerPredictor\" Tip: Run cog init to generate an annotated cog.yaml file that can be used as a starting point for setting up your model. build # This stanza describes how to build the Docker image your model runs in. It contains various options within it: cuda # Cog automatically picks the correct version of CUDA to install, but this lets you override it for whatever reason. gpu # Enable GPUs for this model. When enabled, the nvidia-docker base image will be used, and Cog will automatically figure out what versions of CUDA and cuDNN to use based on the version of Python, PyTorch, and Tensorflow that you are using. For example: build : gpu : true When you use cog run or cog predict , Cog will automatically pass the --gpus=all flag to Docker. When you run a Docker image built with Cog, you'll need to pass this option to docker run . python_packages # A list of Python packages to install, in the format package==version . For example: build : python_packages : - pillow==8.3.1 - tensorflow==2.5.0 python_version # The minor ( 3.8 ) or patch ( 3.8.1 ) version of Python to use. For example: build : python_version : \"3.8.1\" Cog supports all active branches of Python: 3.7, 3.8, 3.9, 3.10. Note that these are the versions supported in the Docker container , not your host machine. You can run any version(s) of Python you wish on your host machine. run # A list of setup commands to run in the environment after your system packages and Python packages have been installed. If you're familiar with Docker, it's like a RUN instruction in your Dockerfile . For example: build : run : - curl -L https://github.com/cowsay-org/cowsay/archive/refs/tags/v3.7.0.tar.gz | tar -xzf - - cd cowsay-3.7.0 && make install Your code is not available to commands in run . This is so we can build your image efficiently when running locally. system_packages # A list of Ubuntu APT packages to install. For example: build : system_packages : - \"ffmpeg\" - \"libavcodec-dev\" image # The name given to built Docker images. If you want to push to a registry, this should also include the registry name. For example: image : \"r8.im/your-username/your-model\" r8.im is Replicate's registry, but this can be any Docker registry. If you don't provide this, a name will be generated from the directory name. predict # The pointer to the cog.Predictor object in your code, which defines how predictions are run on your model. For example: predict : \"predict.py:HotdogPredictor\" See the Python API documentation for more information .","title":"YAML Spec"},{"location":"yaml/#cogyaml-reference","text":"cog.yaml defines how to build a Docker image and how to run predictions on your model inside that image. It has three keys: build , image , and predict . It looks a bit like this: build : python_version : \"3.8\" python_packages : - pytorch==1.4.0 system_packages : - \"ffmpeg\" - \"libavcodec-dev\" predict : \"predict.py:JazzSoloComposerPredictor\" Tip: Run cog init to generate an annotated cog.yaml file that can be used as a starting point for setting up your model.","title":"cog.yaml reference"},{"location":"yaml/#build","text":"This stanza describes how to build the Docker image your model runs in. It contains various options within it:","title":"build"},{"location":"yaml/#cuda","text":"Cog automatically picks the correct version of CUDA to install, but this lets you override it for whatever reason.","title":"cuda"},{"location":"yaml/#gpu","text":"Enable GPUs for this model. When enabled, the nvidia-docker base image will be used, and Cog will automatically figure out what versions of CUDA and cuDNN to use based on the version of Python, PyTorch, and Tensorflow that you are using. For example: build : gpu : true When you use cog run or cog predict , Cog will automatically pass the --gpus=all flag to Docker. When you run a Docker image built with Cog, you'll need to pass this option to docker run .","title":"gpu"},{"location":"yaml/#python_packages","text":"A list of Python packages to install, in the format package==version . For example: build : python_packages : - pillow==8.3.1 - tensorflow==2.5.0","title":"python_packages"},{"location":"yaml/#python_version","text":"The minor ( 3.8 ) or patch ( 3.8.1 ) version of Python to use. For example: build : python_version : \"3.8.1\" Cog supports all active branches of Python: 3.7, 3.8, 3.9, 3.10. Note that these are the versions supported in the Docker container , not your host machine. You can run any version(s) of Python you wish on your host machine.","title":"python_version"},{"location":"yaml/#run","text":"A list of setup commands to run in the environment after your system packages and Python packages have been installed. If you're familiar with Docker, it's like a RUN instruction in your Dockerfile . For example: build : run : - curl -L https://github.com/cowsay-org/cowsay/archive/refs/tags/v3.7.0.tar.gz | tar -xzf - - cd cowsay-3.7.0 && make install Your code is not available to commands in run . This is so we can build your image efficiently when running locally.","title":"run"},{"location":"yaml/#system_packages","text":"A list of Ubuntu APT packages to install. For example: build : system_packages : - \"ffmpeg\" - \"libavcodec-dev\"","title":"system_packages"},{"location":"yaml/#image","text":"The name given to built Docker images. If you want to push to a registry, this should also include the registry name. For example: image : \"r8.im/your-username/your-model\" r8.im is Replicate's registry, but this can be any Docker registry. If you don't provide this, a name will be generated from the directory name.","title":"image"},{"location":"yaml/#predict","text":"The pointer to the cog.Predictor object in your code, which defines how predictions are run on your model. For example: predict : \"predict.py:HotdogPredictor\" See the Python API documentation for more information .","title":"predict"}]}